Optimizers

Optimizers are algorithms or methods used to change the attributes of the neural network such as weights and learning rate to reduce the loss. Basic Stochastic Gradient Descent (SGD) updates parameters using the gradient of the loss with respect to each parameter, calculated from a mini-batch of data.

Advanced optimizers like Adam (Adaptive Moment Estimation) and RMSprop adapt the learning rate during training and provide faster convergence. They are especially effective in handling sparse gradients and noisy data. Choosing the right optimizer often depends on the task and dataset characteristics.